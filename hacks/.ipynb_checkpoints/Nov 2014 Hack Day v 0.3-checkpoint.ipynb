{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd \"/home/kesj/work/nov2014hackday/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from xml.etree.cElementTree import XML\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import XML\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Module that extract text from MS XML Word document (.docx).\n",
    "(Inspired by python-docx <https://github.com/mikemaccana/python-docx>)\n",
    "\"\"\"\n",
    "\n",
    "WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "PARA = WORD_NAMESPACE + 'p'\n",
    "TEXT = WORD_NAMESPACE + 't'\n",
    "\n",
    "\n",
    "def get_docx_text(path):\n",
    "    \"\"\"\n",
    "    Take the path of a docx file as argument, return the text in unicode.\n",
    "    \"\"\"\n",
    "    document = zipfile.ZipFile(path)\n",
    "    xml_content = document.read('word/document.xml')\n",
    "    document.close()\n",
    "    tree = XML(xml_content)\n",
    "\n",
    "    paragraphs = []\n",
    "    for paragraph in tree.getiterator(PARA):\n",
    "        texts = [node.text.encode('ascii','ignore')\n",
    "                 for node in paragraph.getiterator(TEXT)\n",
    "                 if node.text]\n",
    "        if texts:\n",
    "            paragraphs.append(''.join(texts))\n",
    "\n",
    "    return '\\n\\n'.join(paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return float(len(text)) / len(set(text))\n",
    "\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load all the data into \n",
    "all_lines = []\n",
    "\n",
    "fnames = glob.glob('*.docx')\n",
    "print \"Going to process {0} files\".format(len(fnames))\n",
    "for f in fnames:\n",
    "    txt = get_docx_text(f)\n",
    "    lines = [l for l in txt.splitlines() if len(l)>0]\n",
    "    all_lines.extend(lines)\n",
    "\n",
    "len(all_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize.punkt import PunktWordTokenizer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "punkt_tokenizer = PunktWordTokenizer()\n",
    "\n",
    "def tokenize(x):\n",
    "    return [stemmer.stem(s) for s in punkt_tokenizer.tokenize(x)]\n",
    "\n",
    "#bow_matrix = CountVectorizer(tokenizer=tokenize).fit_transform(texts)\n",
    "#normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    "\n",
    "tokenizer=tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import _check_stop_list\n",
    "stoplist = set(_check_stop_list('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents=all_lines\n",
    "texts = [[stemmer.stem(word) for word in (''.join(x for x in document.lower() if x in set('qwertyuiopasdfghjklz xcvbnm|'))).split() \n",
    "          if word not in stoplist] for document in documents]\n",
    "# remove words that appear only once\n",
    "all_tokens = sum(texts)#, [])\n",
    "#from itertools import chain\n",
    "#all_tokens = list(chain.from_iterable(texts))\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word)< (0.01 * len(texts)))\n",
    "my_text = [' '.join([word for word in text if word not in tokens_once]) for text in texts if len(text)>5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_text[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = tokenizer,max_df=.9, min_df=.01, stop_words=stoplist,\n",
    "                             strip_accents=\"ascii\", ngram_range=(0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trans = vectorizer.fit_transform(my_text)#all_lines)\n",
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cluster the bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "import pandas as pd\n",
    "def clusterBullets(X,nclusters=14,random_state=44,show_plot=True):\n",
    "    clstr = cluster.KMeans(random_state=random_state, n_clusters=nclusters)\n",
    "    clstr.fit(X_trans)\n",
    "    if show_plot:\n",
    "        obs_per_cluster = pd.Series(clstr.labels_).value_counts()\n",
    "        obs_per_cluster.plot(kind=\"bar\",color='cadetblue')\n",
    "        ylabel('Number of Observations')\n",
    "        xlabel('Cluster ID')\n",
    "        show()\n",
    "    return clstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc = clusterBullets(X_trans,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine what each group represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def plotGroupDetails(X,cluster,terms,nestimators=100):\n",
    "    nclusters = cluster.n_clusters\n",
    "    X_dense = X.toarray()\n",
    "    y = cluster.labels_\n",
    "    obs_per_cluster = pd.Series(cluster.labels_).value_counts()\n",
    "    for k in range(nclusters):\n",
    "        y_binary = pd.Series(y).replace(k, nestimators)\n",
    "        y_binary = y_binary.replace([e for e in range(nclusters) if e != k], 0)\n",
    "        Forest = RandomForestClassifier(n_estimators=nestimators)\n",
    "        Forest.fit(X_dense, y_binary)\n",
    "        temp = pd.Series(Forest.feature_importances_, index=terms)\n",
    "        temp.sort(ascending=False)\n",
    "        temp[:7].plot(kind=\"bar\", figsize=(8,3), fontsize=14, grid=False, alpha=0.7, linewidth=0.0)\n",
    "        xticks(rotation=50, ha=\"right\")\n",
    "        title(\"Group %d  (n = %d)\" % (k, obs_per_cluster[k]), fontsize=20)\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names() \n",
    "plotGroupDetails(X_trans,cc,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizerLong = TfidfVectorizer(tokenizer = tokenizer,max_df=.9, min_df=.01, stop_words=stoplist,\n",
    "                             strip_accents=\"ascii\", ngram_range=(2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_transLong = vectorizerLong.fit_transform(my_text)\n",
    "X_transLong.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizerLong.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc = clusterBullets(X_transLong,8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = vectorizerLong.get_feature_names() \n",
    "plotGroupDetails(X_transLong,cc,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizerLongB = TfidfVectorizer(tokenizer = tokenizer,max_df=.9, min_df=.01, stop_words=stoplist,\n",
    "                             strip_accents=\"ascii\", ngram_range=(3,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_transLongB = vectorizerLongB.fit_transform(my_text)\n",
    "X_transLongB.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizerLongB.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add stop words\n",
    "more_stop_words = \"\"\"agent dimont fatalities insured s ph phs rd started claim noticed \n",
    "                     n e w south north west east insd insureds went got stated said know \n",
    "                     t policy nan ni took policyholder unsure states like\"\"\".split()\n",
    "stop_words = _check_stop_list('english') | set(more_stop_words)\n",
    "stop_words = set(stop_words)\n",
    "stop_words.remove(\"fire\")\n",
    "vectorizer = TfidfVectorizer(max_df=.9, min_df=50, stop_words=stop_words,\n",
    "                             strip_accents=\"ascii\", ngram_range=(0,2))\n",
    "X_trans = vectorizer.fit_transform(X)\n",
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
