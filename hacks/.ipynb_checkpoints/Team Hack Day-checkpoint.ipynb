{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd \"/home/kesj/work/nov2014hackday/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from xml.etree.cElementTree import XML\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import XML\n",
    "import zipfile\n",
    " \n",
    " \n",
    "\"\"\"\n",
    "Module that extract text from MS XML Word document (.docx).\n",
    "(Inspired by python-docx <https://github.com/mikemaccana/python-docx>)\n",
    "\"\"\"\n",
    " \n",
    "WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "PARA = WORD_NAMESPACE + 'p'\n",
    "TEXT = WORD_NAMESPACE + 't'\n",
    " \n",
    " \n",
    "def get_docx_text(path):\n",
    "    \"\"\"\n",
    "    Take the path of a docx file as argument, return the text in unicode.\n",
    "    \"\"\"\n",
    "    document = zipfile.ZipFile(path)\n",
    "    xml_content = document.read('word/document.xml')\n",
    "    document.close()\n",
    "    tree = XML(xml_content)\n",
    " \n",
    "    paragraphs = []\n",
    "    for paragraph in tree.getiterator(PARA):\n",
    "        texts = [node.text.encode('ascii','ignore')\n",
    "                 for node in paragraph.getiterator(TEXT)\n",
    "                 if node.text]\n",
    "        if texts:\n",
    "            paragraphs.append(''.join(texts))\n",
    " \n",
    "    return '\\n\\n'.join(paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## list files\n",
    "infiles = !ls *\n",
    "infiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notes = []\n",
    "\n",
    "for fname in infiles:\n",
    "    notes.append(get_docx_text(fname))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#txt = get_docx_text(infiles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "notes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into sentences\n",
    "lines = notes[0].split('\\n\\n')\n",
    "print len(lines)\n",
    "\n",
    "words = []\n",
    "for line in lines:\n",
    "    words.append(line.split(' '))\n",
    "\n",
    "from itertools import chain\n",
    "chain = chain.from_iterable(words)\n",
    "txt1 = list(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(txt1), len(txt0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "fd1 = FreqDist(txt0)\n",
    "fd1.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktWordTokenizer\n",
    "#s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "txt0 = PunktWordTokenizer().tokenize(notes[0])\n",
    "txt0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### convert the text into tokens\n",
    "# need to omit  blank lines\n",
    "# need to deal with punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## loading original hackday stuff;;\n",
    "\n",
    "# Add stop words\n",
    "more_stop_words = \"\"\"agent dimont fatalities insured s ph phs rd started claim noticed \n",
    "                     n e w south north west east insd insureds went got stated said know \n",
    "                     t policy nan ni took policyholder unsure states like\"\"\".split()\n",
    "stop_words = _check_stop_list('english') | set(more_stop_words)\n",
    "stop_words = set(stop_words)\n",
    "stop_words.remove(\"fire\")\n",
    "vectorizer = TfidfVectorizer(max_df=.9, min_df=50, stop_words=stop_words,\n",
    "                             strip_accents=\"ascii\", ngram_range=(0,2))\n",
    "X_trans = vectorizer.fit_transform(X)\n",
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = txt.splitlines()\n",
    "lines\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonblank = [l for l in lines if len(l) >0]\n",
    "#team = []\n",
    "team = [l for l in nonblank if len(l.split(' '))==2]\n",
    "team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktWordTokenizer\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "txt2 = PunktWordTokenizer().tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "wordCount = defaultdict(int)\n",
    "for word in txt2:\n",
    "    wordCount[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist(wordCount.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    " \n",
    "from nltk.tokenize.api import StringTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "def parseDocument(document):\n",
    "    sentence_tokenizer = StringTokenizer()\n",
    "    sentence_tokenizer._string = '\\n'\n",
    "    sentences = sentence_tokenizer.tokenize(document)\n",
    "    print len(sentences)\n",
    "    \n",
    "    return enumerate(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=.9, min_df=50, stop_words=stop_words,\n",
    "                             strip_accents=\"ascii\", ngram_range=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = parseDocument(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    " \n",
    "from nltk.tokenize.api import StringTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    " \n",
    "def textrank(document):\n",
    "    sentence_tokenizer = StringTokenizer()\n",
    "    sentence_tokenizer._string = '\\n'\n",
    "    sentences = sentence_tokenizer.tokenize(document)\n",
    " \n",
    "    bow_matrix = CountVectorizer().fit_transform(sentences)\n",
    "    normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    " \n",
    "    similarity_graph = normalized * normalized.T\n",
    " \n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    return sorted(((scores[i],s) for i,s in enumerate(sentences)),\n",
    "                  reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textrank(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# file parser\n",
    "# assume the 1st line has title and date\n",
    "titleline = lines[0]\n",
    "titleline\n",
    "# pull out lines that are length 0\n",
    "\n",
    "# pull out lines that have 2 terms (i.e. the team name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import _check_stop_list\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def new_euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False):\n",
    "    return cosine_similarity(X,Y)\n",
    "\n",
    "from sklearn.cluster import k_means_\n",
    "k_means_.euclidean_distances = new_euclidean_distances\n",
    "\n",
    "from sklearn import cluster\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def eval_accuracy(y_test, y_pred):\n",
    "    figsize(7,7)\n",
    "    print \"\\nPercent correct = %0.2f%%\\n\" % (accuracy_score(y_test, y_pred)*100)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    matshow(cm)\n",
    "    set_cmap('Blues')\n",
    "    text(4.5, -1.5, \"Confusion matrix\",\n",
    "             horizontalalignment=\"center\",\n",
    "             fontsize=23)\n",
    "    colorbar()\n",
    "    ylabel('True label', fontsize=15)\n",
    "    xlabel('Predicted label', fontsize=15)\n",
    "    #tight_layout()\n",
    "    show()\n",
    "    print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_raw = txt\n",
    "X = X.LOS_DESC_TXT.apply(str)\n",
    "X[X.isnull() == True] = \"Missing\"\n",
    "X = X.apply(lambda s: ''.join(x for x in s.lower() if x in set('qwertyuiopasdfghjklz xcvbnm|')))\n",
    "\n",
    "def replace_words(s):\n",
    "    new_text = []\n",
    "    for e in s.split():\n",
    "        if e.lower() == \"home\":\n",
    "            new_text.append(\"house\")\n",
    "        else:\n",
    "            new_text.append(e)\n",
    "    return \" \".join(new_text)\n",
    "X = X.apply(replace_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    " \n",
    "from nltk.tokenize.api import StringTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    " \n",
    "def textrank(document):\n",
    "    sentence_tokenizer = StringTokenizer()\n",
    "    sentence_tokenizer._string = '\\n'\n",
    "    sentences = sentence_tokenizer.tokenize(document)\n",
    " \n",
    "    bow_matrix = CountVectorizer().fit_transform(sentences)\n",
    "    normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    " \n",
    "    similarity_graph = normalized * normalized.T\n",
    " \n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    return sorted(((scores[i],s) for i,s in enumerate(sentences)),\n",
    "                  reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textrank('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Sample TextRank code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.data.path.append('/home/kes1/data/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From this paper: http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Mihalcea.pdf \n",
    "\n",
    "I used python with nltk, and pygraph to do an implmentation of of textrank.\n",
    "\n",
    "for questions: http://twitter.com/voidfiles\n",
    "\n",
    "\"\"\"\n",
    "import nltk\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    " \n",
    "from pygraph.classes.graph import graph\n",
    "from pygraph.classes.digraph import digraph\n",
    "from pygraph.algorithms.pagerank import pagerank\n",
    "from pygraph.classes.exceptions import AdditionError\n",
    " \n",
    "text = u\"\"\"In the Valley, we have lots of euphemisms for changing your business until you find a way to make money: You might throw things at the wall and see what sticks, or go where the money is, or pivot. Whatever you call it, it boils down to a basic animal instinct-the killer instinct. You need to look at the whole picture and attack an area that is vulnerable, and then keep attacking until you have won, or until you find an even more vulnerable spot. Rinse, then repeat.\n",
    "\n",
    "I have yet to run my own company, but that doesn't stop me from evaluating the ability of a business to harness its killer instinct and fuel its own expansion. I have worked for companies with and without this instinct. I like working for companies with a keen killer instinct.\n",
    "\n",
    "This killer instinct directly relates to last month's Google Reader debacle. I would often deride Google for changing Reader, but at the same time, I knew from the beginning that it was the right move on the part of Google.\n",
    "\n",
    "Google has amassed their resources to support Google+. They have gone so far as to tie employees' salaries and bonuses to how well Google+ does. They then rolled out integrations across the company. The company uses anything that could possibly prop up Google+ to drive the success of the project. This is the killer instinct in action. Google knows that if they don't combat Facebook, they are going to forfeit a significant market in the future. They aren't going to lose this battle without a fight.\n",
    "\n",
    "As an outsider, and as a former Yahoo employee, I applaud Google's determination. Yahoo had been trying to start a social networking service for as long as I worked there. The problem with the Yahoo social networking plan is that they have tried five5 different things in five5 years. Apparently Google+ wasn't all that welcome at Google in it's internal beta, and there have even been some very public rants from Googlers about the faults of Google+,the project- but Google is still pushing it hard. If Yahoo ran had run into this much resistance, they would have shut it down.\n",
    "\n",
    "Now that I work for a small company, I have had the chance to see killer instinct in the flesh. I know how much focus it gives a company, and that it drives the development of a strong plan. It gives you a roadmap, even when you don't always know what the future looks like. I can only hope that when I run my own company, I'll have that same killer instinct.\"\"\"\n",
    " \n",
    " \n",
    "text = nltk.word_tokenize(text)\n",
    " \n",
    "tagged = nltk.pos_tag(text)\n",
    " \n",
    " \n",
    "def filter_for_tags(tagged, tags=['NN', 'JJ', 'NNP']):\n",
    "    return [item for item in tagged if item[1] in tags]\n",
    " \n",
    " \n",
    "def normalize(tagged):\n",
    "    return [(item[0].replace('.', ''), item[1]) for item in tagged]\n",
    " \n",
    " \n",
    "def unique_everseen(iterable, key=None):\n",
    "    \"List unique elements, preserving order. Remember all elements ever seen.\"\n",
    "    # unique_everseen('AAAABBBCCDAABBB') --> A B C D\n",
    "    # unique_everseen('ABBCcAD', str.lower) --> A B C D\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    if key is None:\n",
    "        for element in itertools.ifilterfalse(seen.__contains__, iterable):\n",
    "            seen_add(element)\n",
    "            yield element\n",
    "    else:\n",
    "        for element in iterable:\n",
    "            k = key(element)\n",
    "            if k not in seen:\n",
    "                seen_add(k)\n",
    "                yield element\n",
    " \n",
    "tagged = filter_for_tags(tagged)\n",
    "tagged = normalize(tagged)\n",
    " \n",
    "unique_word_set = unique_everseen([x[0] for x in tagged])\n",
    " \n",
    "gr = digraph()\n",
    "gr.add_nodes(list(unique_word_set))\n",
    " \n",
    "window_start = 0\n",
    "window_end = 2\n",
    " \n",
    "while 1:\n",
    " \n",
    "    window_words = tagged[window_start:window_end]\n",
    "    if len(window_words) == 2:\n",
    "        print window_words\n",
    "        try:\n",
    "            gr.add_edge((window_words[0][0], window_words[1][0]))\n",
    "        except AdditionError, e:\n",
    "            print 'already added %s, %s' % ((window_words[0][0], window_words[1][0]))\n",
    "    else:\n",
    "        break\n",
    " \n",
    "    window_start += 1\n",
    "    window_end += 1\n",
    " \n",
    "calculated_page_rank = pagerank(gr)\n",
    "di = sorted(calculated_page_rank.iteritems(), key=itemgetter(1))\n",
    "for k, g in itertools.groupby(di, key=itemgetter(1)):\n",
    "    print k, map(itemgetter(0), g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tagged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
