{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import _check_stop_list\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd \"/home/kesj/work/nov2014hackday/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to parse the data\n",
    "try:\n",
    "    from xml.etree.cElementTree import XML\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import XML\n",
    "import zipfile\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Module that extract text from MS XML Word document (.docx).\n",
    "(Inspired by python-docx <https://github.com/mikemaccana/python-docx>)\n",
    "\"\"\"\n",
    "\n",
    "WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "PARA = WORD_NAMESPACE + 'p'\n",
    "TEXT = WORD_NAMESPACE + 't'\n",
    "\n",
    "\n",
    "def get_docx_text(path):\n",
    "    \"\"\"\n",
    "    Take the path of a docx file as argument, return the text in unicode.\n",
    "    \"\"\"\n",
    "    document = zipfile.ZipFile(path)\n",
    "    xml_content = document.read('word/document.xml')\n",
    "    document.close()\n",
    "    tree = XML(xml_content)\n",
    "\n",
    "    paragraphs = []\n",
    "    for paragraph in tree.getiterator(PARA):\n",
    "        #texts = [node.text\n",
    "        texts = [node.text.encode('ascii','ignore')\n",
    "                 for node in paragraph.getiterator(TEXT)\n",
    "                 if node.text]\n",
    "        if texts:\n",
    "            paragraphs.append(''.join(texts))\n",
    "\n",
    "    return '\\n\\n'.join(paragraphs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This version pushes the data into a data frame \n",
    "The format of the data frame is 4 coulumns: date, ,Number of teams, Number of bullet points, and text where text is a dictionary that groups the bullet points under each team\n",
    "\n",
    "TODO: rework the get_docx_text file so that it parses the XML tree structure directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "fnames = glob.glob('*.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing --> work with a single file at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"working with the file: \",fnames[3]\n",
    "#original get_docx parsing:\n",
    "txt3 = get_docx_text(fnames[3])\n",
    "print len(txt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseFileText(txt):\n",
    "    gnameThresh = 5\n",
    "    # first pull out the file name and date\n",
    "    matchObj = re.match( r'(Weekly Staff Notes\\s\\s)(\\S+\\s?\\S+,\\s2014)(.*)', txt, re.M|re.I|re.DOTALL)\n",
    "    title = matchObj.group(1)\n",
    "    date = matchObj.group(2)\n",
    "    rest = matchObj.group(3)\n",
    "    #print date \n",
    "    # next parse the rest into the number of teams/subteams and number of bullet points.\n",
    "    lines = [l for l in rest.splitlines() if len(l)>0] # keep only non-zero length lines\n",
    "    groups = [k for k in lines if len(k.split(' '))< gnameThresh]\n",
    "    ngroups = len(groups)\n",
    "    #print ngroups, len(lines)\n",
    "    #if ngroups > 5:\n",
    "    #    print groups\n",
    "    \n",
    "    # note for this case I need to join groups 1 & 2 to the initial one\n",
    "    kgrp=0\n",
    "    teamLines = {} #dictionary of lists\n",
    "    \n",
    "    for line in lines:\n",
    "        if line == groups[kgrp]:\n",
    "            #print kgrp, gg[kgrp]\n",
    "            if kgrp >0:\n",
    "                teamLines[groups[kgrp-1]]=teamLine\n",
    "            teamLine = [] #reinitialize it\n",
    "            if kgrp < len(groups)-1:\n",
    "                kgrp+=1\n",
    "        else:\n",
    "            teamLine.append(line)\n",
    "\n",
    "    teamLines[groups[kgrp]]=teamLine\n",
    "    nbullets = len(lines)-ngroups\n",
    "    \n",
    "    #print date,ngroups,nbullets\n",
    "   \n",
    "    \n",
    "    return (date,groups,teamLines)\n",
    "    #return parsedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def docDF():\n",
    "    df = pd.DataFrame(columns=['Date','Team_Count','Bullet_Count','text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myDF = docDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bulletLines = sum(TeamLines.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myDF = docDF()\n",
    "for f in fnames:\n",
    "    #print f\n",
    "    txt = get_docx_text(f)\n",
    "    try:\n",
    "        date,grps,TeamLines = parseFileText(txt)\n",
    "        mylen = len(myDF)\n",
    "        myDF.loc[mylen,'Date']= date\n",
    "        myDF.loc[mylen,'Team_Count'] = len(grps)\n",
    "        myDF.loc[mylen,'Bullet_Count']=sum([len(v) for v in TeamLines.values()])\n",
    "        myDF.loc[mylen,'text']=TeamLines\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#reindex as time series\n",
    "newIDX = myDF['Date'].apply(lambda x: pd.to_datetime(x))\n",
    "myDF.index = newIDX\n",
    "print np.shape(myDF)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myDF.Bullet_Count.plot()\n",
    "myDF.Team_Count.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myDF['2014-02-28'].Bullet_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myDF['2014-02-28'].text.values[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myDF.Team_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newIDX = myDF['Date'].apply(lambda x: pd.to_datetime(x))\n",
    "myDF.index = newIDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_tmp = myDF['2014-09-26'].text.values\n",
    "text_tmp[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myDF.text.values[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print [myDF.text.values[a].keys() for a in xrange(0,len(myDF))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for a given document look at all bullet point text \n",
    "myDoc = sum(myDF.text.values[0].values())\n",
    "len(myDoc), myDF.ix[0].Bullet_Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process all the text as tokens\n",
    "\n",
    "1. stem words, convert to lower, remove non letters, & remove stop-words (using PunktTokenizer)\n",
    "2. all_tokens is a list of all the 'words'\n",
    "3. remove words that only occur less than 1 % of the time\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [[stemmer.stem(word) for word in (''.join(x for x in document.lower() if x in set('qwertyuiopasdfghjklz xcvbnm|'))).split() if word not in stoplist] for document in myDoc]\n",
    "# remove words that appear only once\n",
    "all_tokens = sum(texts)#, [])\n",
    "#from itertools import chain\n",
    "#all_tokens = list(chain.from_iterable(texts))\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word)< (0.01 * len(texts)))\n",
    "#if len(tokens_once)>0:\n",
    "    \n",
    "my_text = [[word for word in text if word not in tokens_once] for text in texts if len(text)>5]\n",
    "print shape(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count = Counter(all_tokens)\n",
    "print count.most_common(10)\n",
    "hist(count.values(),color='deeppink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return float(len(text)) / len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myDF['text'].apply(lambda x: lexical_diversity(list(flatten(x.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myDF['text'].apply(lambda x: lexical_diversity(x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#t2 = sum(my_text)\n",
    "X_trans = vectorizer.fit_transform(t2)\n",
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "all_lines = []\n",
    "\n",
    "fnames = glob.glob('*.docx')\n",
    "for f in fnames:\n",
    "    txt = get_docx_text(f)\n",
    "    lines = [l for l in txt.splitlines() if len(l)>0]\n",
    "    all_lines.extend(lines)\n",
    "\n",
    "len(all_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get 1st word for each line:\n",
    "from collections import defaultdict\n",
    "lineWordCount = defaultdict(int)\n",
    "for l in all_lines:\n",
    "    wordKey = l.split(' ')[0]\n",
    "    lineWordCount[wordKey]+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[ len(a) for a in all_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import _check_stop_list\n",
    "stoplist = set(_check_stop_list('english'))\n",
    "print len(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize.punkt import PunktWordTokenizer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "punkt_tokenizer = PunktWordTokenizer()\n",
    "\n",
    "def tokenize(x):\n",
    "    return [stemmer.stem(s) for s in punkt_tokenizer.tokenize(x)]\n",
    "\n",
    "#bow_matrix = CountVectorizer(tokenizer=tokenize).fit_transform(texts)\n",
    "#normalized = TfidfTransformer().fit_transform(bow_matrix)\n",
    "\n",
    "tokenizer=tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents=all_lines\n",
    "texts = [[stemmer.stem(word) for word in (''.join(x for x in document.lower() if x in set('qwertyuiopasdfghjklz xcvbnm-'))).split() \n",
    "          if word not in stoplist] for document in documents]\n",
    "\n",
    "print shape(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stoplist = set(._check_stop_list('english'))\n",
    "documents=all_lines\n",
    "texts = [[stemmer.stem(word) for word in (''.join(x for x in document.lower() if x in set('qwertyuiopasdfghjklz xcvbnm|'))).split() if word not in stoplist] for document in documents]\n",
    "# remove words that appear only once\n",
    "all_tokens = sum(texts)#, [])\n",
    "#from itertools import chain\n",
    "#all_tokens = list(chain.from_iterable(texts))\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word)< (0.01 * len(texts)))\n",
    "my_text = [[word for word in text if word not in tokens_once] for text in texts if len(text)>5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CV_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = tokenizer,max_df=.9, min_df=0.01, stop_words=stoplist,\n",
    "                             strip_accents=\"ascii\", ngram_range=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_trans=CV_vectorizer.fit_transform(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in (''.join(x for x in document.lower() if x in set('qwertyuiopasdfghjklz xcvbnm-|'))).split() if word not in stoplist] for document in all_lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in (''.join(x for x in document.lower() if x in set('qwertyuiopasdfghjklz xcvbnm-|'))).split() if word not in stoplist] for document in all_lines]\n",
    "# remove words that appear only once\n",
    "all_tokens = sum(texts)\n",
    "#tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word)< (0.01 * len(texts)))\n",
    "\n",
    "texts = [[word for word in text if word not in tokens_once] for text in texts]\n",
    "\n",
    "#tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lineWordCount.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = tokenizer,max_df=.9, min_df=.01, stop_words=stoplist,\n",
    "                             strip_accents=\"ascii\", ngram_range=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trans = vectorizer.fit_transform(my_text)\n",
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# group the bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "import pandas as pd\n",
    "def clusterBullets(X,nclusters=14,random_state=44,show_plot=True):\n",
    "    clstr = cluster.KMeans(random_state=random_state, n_clusters=nclusters)\n",
    "    clstr.fit(X_trans)\n",
    "    if show_plot:\n",
    "        obs_per_cluster = pd.Series(clstr.labels_).value_counts()\n",
    "        obs_per_cluster.plot(kind=\"bar\",color='cadetblue')\n",
    "        ylabel('Number of Observations')\n",
    "        xlabel('Cluster ID')\n",
    "        show()\n",
    "    return clstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cc = clusterBullets(X_trans,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine what each group represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names() \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def plotGroupDetails(X,cluster,terms,nestimators=100):\n",
    "    nclusters = cluster.n_clusters\n",
    "    X_dense = X.toarray()\n",
    "    y = cluster.labels_\n",
    "    obs_per_cluster = pd.Series(cluster.labels_).value_counts()\n",
    "    for k in range(nclusters):\n",
    "        y_binary = pd.Series(y).replace(k, nestimators)\n",
    "        y_binary = y_binary.replace([e for e in range(nclusters) if e != k], 0)\n",
    "        Forest = RandomForestClassifier(n_estimators=nestimators)\n",
    "        Forest.fit(X_dense, y_binary)\n",
    "        temp = pd.Series(Forest.feature_importances_, index=terms)\n",
    "        temp.sort(ascending=False)\n",
    "        temp[:7].plot(kind=\"bar\", figsize=(8,3), fontsize=14, grid=False, alpha=0.7, linewidth=0.0)\n",
    "        xticks(rotation=50, ha=\"right\")\n",
    "        title(\"Group %d  (n = %d)\" % (k, obs_per_cluster[k]), fontsize=20)\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotGroupDetails(X_trans,cc,terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look at cosine similarity between bullet points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "np.shape(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosSIM_bullets = cosine_similarity(X_trans,X_trans)\n",
    "np.shape(cosSIM_bullets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.__version__\n",
    "sns.heatmap(cosSIM_bullets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try pca on the vectorized dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df = pd.read_csv('HACK_DAY_DATA.csv', infer_datetime_format=True, \n",
    "                     dtype={'RSRV_COL_CD': str, 'CLM_ID': str, 'LOS_DESC_TXT':str}) \n",
    "X_raw = X.copy()\n",
    "X = X.LOS_DESC_TXT.apply(str)\n",
    "X[X.isnull() == True] = \"Missing\"\n",
    "X = X.apply(lambda s: ''.join(x for x in s.lower() if x in set('qwertyuiopasdfghjklz xcvbnm|')))\n",
    "\n",
    "def replace_words(s):\n",
    "    new_text = []\n",
    "    for e in s.split():\n",
    "        if e.lower() == \"home\":\n",
    "            new_text.append(\"house\")\n",
    "        else:\n",
    "            new_text.append(e)\n",
    "    return \" \".join(new_text)\n",
    "X = X.apply(replace_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use PUNKT tokenizer\n",
    "from nltk.tokenize.punkt import PunktWordTokenizer\n",
    "#s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "txt0b = PunktWordTokenizer().tokenize(notes2[0])\n",
    "txt0b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_claim_1 = X_raw[\"LOS_DESC_TXT\"][1]\n",
    "example_claim_2 = X_raw[\"LOS_DESC_TXT\"][5]\n",
    "\n",
    "print example_claim_1 + \"\\n\"\n",
    "print example_claim_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add stop words\n",
    "more_stop_words = \"\"\"agent dimont fatalities insured s ph phs rd started claim noticed \n",
    "                     n e w south north west east insd insureds went got stated said know \n",
    "                     t policy nan ni took policyholder unsure states like\"\"\".split()\n",
    "stop_words = _check_stop_list('english') | set(more_stop_words)\n",
    "stop_words = set(stop_words)\n",
    "stop_words.remove(\"fire\")\n",
    "vectorizer = TfidfVectorizer(max_df=.9, min_df=50, stop_words=stop_words,\n",
    "                             strip_accents=\"ascii\", ngram_range=(0,2))\n",
    "X_trans = vectorizer.fit_transform(X)\n",
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically group claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clstr = cluster.KMeans(random_state=42, n_clusters=14)\n",
    "clstr.fit(X_trans)\n",
    "obs_per_cluster = pd.Series(clstr.labels_).value_counts()\n",
    "obs_per_cluster.plot(kind=\"bar\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts2 = [''.join(x for x in document.lower() if x in set('qwertyuiopasdfghjklz xcvbnm-')) for document in all_lines]\n",
    "#shape(texts2), shape(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def myVectorizor()\n",
    "vectorizer2 = TfidfVectorizer(max_df=.9, min_df=.01, stop_words=stoplist,\n",
    "                             strip_accents=\"ascii\", ngram_range=(0,5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for a in texts2:\n",
    "    print a,len(a.split(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trans = vectorizer2.fit_transform(texts2)\n",
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trans[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cluster the bullets\n",
    "clstr = cluster.KMeans(random_state=42, n_clusters=14)\n",
    "clstr.fit(X_trans)\n",
    "obs_per_cluster = pd.Series(clstr.labels_).value_counts()\n",
    "obs_per_cluster.plot(kind=\"bar\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lineWordCount.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign meaningful labels to each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_for_data = {0:\"lawn mower\",\n",
    "                   1:\"garage fire\",\n",
    "                   2:\"various claims\",\n",
    "                   3:\"boat\",\n",
    "                   4:\"pool\",\n",
    "                   5:\"smoke damage\",\n",
    "                   6:\"water damage\",\n",
    "                   7:\"total loss\",\n",
    "                   8:\"personal property\",\n",
    "                   9:\"house fire\",\n",
    "                   10:\"jewelry\",\n",
    "                   11:\"fire\",\n",
    "                   12:\"ac unit\",\n",
    "                   13:\"burned\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot unusual trends by tracking groups through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_index = X_raw.LOS_SBMT_TSTMP.apply(lambda x: pd.to_datetime(\"01/%s/%s\" % (x[2:5], x[5:9])))\n",
    "X_time = pd.DataFrame(clstr.labels_, index=time_index)\n",
    "X_time.sort_index(inplace=True)\n",
    "temp = pd.DataFrame(X_time)\n",
    "grouped = pd.get_dummies(temp[0]).groupby(by=temp.index)\n",
    "month_count = pd.DataFrame()\n",
    "\n",
    "for e, frame in grouped:\n",
    "    month_count[e] = frame.sum()\n",
    "month_count = month_count.T\n",
    "\n",
    "for col in month_count.columns:\n",
    "    month_count[col][8:].plot(figsize=(14,5), c=\"k\", linewidth=3, fontsize=12)\n",
    "    title(labels_for_data[col], fontsize=14)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly detect unexpected claims events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(100, oob_score=True)\n",
    "clf.fit(X_dense, y)\n",
    "clf.oob_decision_function_\n",
    "y_hat_oob = [[e[0] for e in enumerate(scores) if e[1] == max(scores)][0] for scores in clf.oob_decision_function_]\n",
    "eval_accuracy(y, y_hat_oob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print example_claim_1 + \"\\n\"\n",
    "print example_claim_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_claim_type(claim_description):\n",
    "    temp = vectorizer.transform(pd.Series(claim_description)).toarray() \n",
    "    print \"Claim description: %s \\n\" % claim_description\n",
    "    print \"Predicted group: %s\" % labels_for_data[int(clf.predict(temp))]\n",
    "    pd.Series(clf.predict_proba(temp)[0], labels_for_data.values()).plot(kind=\"barh\", figsize=(10,5))\n",
    "    show()\n",
    "    print \"\\n\"\n",
    "    \n",
    "predict_claim_type(example_claim_1)\n",
    "predict_claim_type(example_claim_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_claim_type(\"Lalalallala. What happened! No one has any idea!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "week_id = [(1, 2012), (2, 2012), (3, 2012), \"...\"]\n",
    "\n",
    "actual_categories = [\"FIRE\", \"ANIMALS\", \"...\", \"UNKNOWN\"]   # Coop\n",
    "\n",
    "list_of_categories = [{\"claim_type\": \"Pool\", \n",
    "                       \"claim_key_words\": [\"pool\", \"ground\", \"water\"], # Coop\n",
    "                       \"key_word_importance\": [100, 90, 80], # Coop\n",
    "                       \"counts_per_week\": [43, 54, 12, 12], # Coop\n",
    "                       \"UCL\": [47, 57, 14, 10],\n",
    "                       \"LCL\": [40, 50, 10, 8],\n",
    "                       \"total_counts\": 119, # Coop\n",
    "                       \"example_claims\":[\"pool stuff happened\", \"The POOL exploded\", \"...\"], # Coop -- max examples\n",
    "                       \"distribution_over_actual_labels\":[0, 0, \"...\", 119]}, # Coop\n",
    "                       \n",
    "                       {\"claim_type\": \"Boat\",\n",
    "                        \"...\"\n",
    "                        }                      \n",
    "                      ]\n",
    "                       \n",
    "category_importance = [\"Pool\", \"Boat\", \"Garage Fire\", \"Lawn Mower\", \"Smoke Damage\", \"Jewelry\", \"...\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_data = ['21467', '21256', '20917', '20866', '20934', '21062', '20895', '20813', '20964', '21231', '21183', '21046', '21038', '20982', '20722', '20503', '20286', '20236', '20227', '20396', '20481', '20627', '20758', '20872', '20879', '20917', '20601', '20503', '20435', '20504', '20490', '20607', '20708', '20743', '21018', '21197', '21066', '20827', '20724', '20757', '20851', '20977', '21079', '21229', '21192', '21417', '21338', '21304', '21166', '21118', '21066', '20997', '21140', '21360', '21491', '21477', '21746', '21904', '22005', '22421']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = [int(x) for x in sample_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_predictions(y):\n",
    "    \"\"\"y: list like object\"\"\"\n",
    "    base_x = np.arange(0, len(y))\n",
    "    X = pd.DataFrame(base_x, columns=[\"x\"])\n",
    "    X[\"x^2\"] = base_x**2\n",
    "    X[\"x^3\"] = base_x**3\n",
    "\n",
    "    base_month = base_x % 12\n",
    "    for element in range(12):\n",
    "        X[\"d\" + str(element)] = (base_month == element)*1\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    X_p = lr.predict(X)\n",
    "    return X_p\n",
    "\n",
    "def get_CIs(y):\n",
    "    \"\"\"returns UCL and LCL\"\"\"\n",
    "    X_p = get_predictions(y)\n",
    "    n_standard_deviations = 1.96\n",
    "    standard_deviation = (((y - X_p)**2).sum()/(len(y)-1))**(.5)\n",
    "    UCL = X_p + standard_deviation * n_standard_deviations\n",
    "    LCL = X_p - standard_deviation * n_standard_deviations\n",
    "    return UCL, LCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_predictions_weekly(y):\n",
    "    \"\"\"y: list like object\"\"\"\n",
    "    base_x = np.arange(0, len(y))\n",
    "    X = pd.DataFrame(base_x, columns=[\"x\"])\n",
    "    X[\"x^2\"] = base_x**2\n",
    "    X[\"x^3\"] = base_x**3\n",
    "    base_month = base_x % 12\n",
    "    base = (base_x / 4 ) % 13\n",
    "    \n",
    "    for element in range(12):\n",
    "        X[\"d\" + str(element)] = (base == element)*1\n",
    "        \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    X_p = lr.predict(X)\n",
    "    return X_p\n",
    "\n",
    "def get_CIs_weekly(y):\n",
    "    \"\"\"\n",
    "    y is a list like object. (Counts through time)\n",
    "    returns UCL, LCL, and importance of last point\n",
    "    \"\"\"\n",
    "    X_p = get_predictions_weekly(y)\n",
    "    n_standard_deviations = 1.96\n",
    "    standard_deviation = (((y - X_p)**2).sum()/(len(y)-1))**(.5)\n",
    "    error_of_last_point = np.abs(y[-1:] - X_p[-1:])\n",
    "    standardized_error_of_last_point = error_of_last_point/standard_deviation\n",
    "    UCL = X_p + standard_deviation * n_standard_deviations\n",
    "    LCL = X_p - standard_deviation * n_standard_deviations\n",
    "    return UCL, LCL, standardized_error_of_last_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UCL, LCL, importance_of_last_point = get_CIs_weekly(y)\n",
    "\n",
    "print UCL\n",
    "print LCL\n",
    "print importance_of_last_point\n",
    "\n",
    "plot(UCL)\n",
    "plot(y)\n",
    "plot(LCL)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json_data=open(\"data.json\")\n",
    "data = json.load(json_data)\n",
    "json_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "    \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "        \n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    "    \n",
    "    see also: \n",
    "    \n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    " \n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError, \"smooth only accepts 1 dimension arrays.\"\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError, \"Input vector needs to be bigger than window size.\"\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError, \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\"\n",
    "\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-1:-window_len:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "better_dates = [\"\"]+[\"%s:%s\" % (str(date[1]), str(date[0])[2:]) if i%2==0 else \"\" for i, date in enumerate(data[\"week_id\"])]\n",
    "print better_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = [u'hail damage', u'hail damage roof', u'damage roof', u'wind hail damage', u'wind hail', u'wind hail damage roof', u'possible hail damage', u'possible hail', u'possible', u'possible hail damage roof']\n",
    "importances = [8.223633045300254, 6.632623373024898, 5.96108359745089, 0.5642348824945708, 0.48056411438294433, 0.47280524975865235, 0.3153134637741829, 0.3048362127242999, 0.27528449640413066, 0.2511159452864281]\n",
    "\n",
    "# Filter out unimportant words\n",
    "importances = np.array(importances)\n",
    "importances /= importances.sum()\n",
    "\n",
    "total = 0\n",
    "term_count = 0\n",
    "for e in importances:\n",
    "    total += e\n",
    "    term_count += 1\n",
    "    if total >= .8:\n",
    "        break\n",
    "        \n",
    "terms = terms[:term_count]\n",
    "\n",
    "terms_to_skip = []\n",
    "\n",
    "for main_element in terms:\n",
    "    for second_element in terms:\n",
    "        if main_element == second_element:\n",
    "            continue\n",
    "        if main_element in second_element:\n",
    "            print main_element, \":\", second_element\n",
    "            terms_to_skip.append(main_element)\n",
    "            break\n",
    "\n",
    "terms_to_skip\n",
    "\n",
    "cat_name = filter(lambda x: x not in terms_to_skip, terms)\n",
    "cat_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_category_importance = []\n",
    "\n",
    "names_of_categories = {\n",
    "0:'Hail',\n",
    "1:'Wind',\n",
    "2:'Internal water',\n",
    "3:'Tree related',\n",
    "4:'Shingles',\n",
    "5:'Fire',\n",
    "6:'Personal property theft',\n",
    "7:'Gutters',\n",
    "8:'Storms',\n",
    "9:'Basement',\n",
    "10:\"Hail\",\n",
    "11:\"Roof\",\n",
    "12:\"Siding\",\n",
    "13:\"Garage door\"\n",
    "}\n",
    "\n",
    "for claim_category in data['list_of_categories']:\n",
    "    c = claim_category['claim_id']\n",
    "    claim_category[\"counts_per_week\"] = claim_category[\"counts_per_week\"][:-2]\n",
    "    category_name = names_of_categories[c]\n",
    "    print category_name\n",
    "    claim_category['claim_type'] = category_name\n",
    "    print claim_category[\"claim_key_words\"][:]\n",
    "    print claim_category['key_word_importance'][:]\n",
    "    counts = claim_category[\"counts_per_week\"]\n",
    "    UCL, LCL, importance_of_last_point = get_CIs_weekly(counts)\n",
    "    figsize(20,5)\n",
    "    UCL = list(smooth(UCL, window_len=6)[4:])\n",
    "    LCL = list(smooth(LCL, window_len=6)[4:])\n",
    "    claim_category[\"UCL\"] = UCL\n",
    "    claim_category[\"LCL\"] = LCL\n",
    "    claim_category[\"category_importance\"] = importance_of_last_point[0]\n",
    "    temp_category_importance.append([category_name, importance_of_last_point[0]])\n",
    "    plot(counts)\n",
    "    plot(UCL, \"k\")\n",
    "    plot(LCL, \"k\")\n",
    "    x = range(len(counts))\n",
    "    xticks(x, better_dates, rotation=90)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_category_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "print temp_category_importance\n",
    "print [e[0] for e in temp_category_importance]\n",
    "data['category_importance'] = [e[0] for e in temp_category_importance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"data2.json\", \"wb\") as json_writer:\n",
    "    json.dump(data, json_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat data2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
